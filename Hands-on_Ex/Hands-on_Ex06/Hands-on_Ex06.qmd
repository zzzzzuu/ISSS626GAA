---
title: "Hands-on Exercise 6"
author: "Leow Xian Zu"
date: "23 Sep 2024"
date-modified: "last-modified"
execute:
  eval: true
  echo: true 
  message: false #no more warning message
  freeze: true #whatever document already commited does not render
---

# Geographical Segmentation with Spatially Constrained Clustering Techniques

Geographical segmentation using spatially constrained clustering techniques is important for several reasons.

1\. Spatial continuity:

Traditional clustering methods might group similar areas that are far apart. In many real-world applications, we need contiguous regions.

2\. Respect for geographical boundaries:

Natural or administrative boundaries often matter in analysis and decision-making.

3\. Interpretability:

Spatially coherent clusters are often easier to understand and explain, especially in fields like urban planning or market analysis.

4\. Practical applicability:

For things like service area planning or resource allocation, contiguous regions are more manageable.

5\. Capturing local context:

Nearby areas often share similar characteristics due to shared environmental, social, or economic factors.

6\. Reducing noise:

Spatial constraints can help filter out random variations and focus on meaningful patterns.

7\. Compliance with regulations:

Some applications (e.g., electoral districting) legally require spatial contiguity.

8\. Improved prediction:

In some cases, spatially constrained clusters can lead to better predictive models by accounting for spatial dependencies.

Let's explore this.

# Set up

## Load packages

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
set.seed(1234)
```

## Read data

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))
shan_sf
glimpse(shan_sf)
```

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
summary(ict)
```

## Data wrangling

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
summary(ict_derived)
```

# Exploratory Data Analysis

## Data analysis

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

## Chloropleth data analysis

```{r}
shan_sf <- left_join(shan_sf, 
                     ict_derived, by=c("TS_PCODE"="TS_PCODE"))
  
write_rds(shan_sf, "data/rds/shan_sf.rds")
```

```{r}
shan_sf <- read_rds("data/rds/shan_sf.rds")
qtm(shan_sf, "RADIO_PR")
```

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

# Correlation analysis

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

# Hierarchy Cluster Analysis

## 1) **Extracting clustering variables**

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

## 2) Data standardisation

## 3) Min-Max standardisation

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

## Z-Score Standardisation

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

## Visualising standardised clustering variables

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

The histograms shown provide a visual comparison of data distribution before and after two different standardization techniques: Min-Max Standardization and Z-score Standardization. Here are the key statistical conclusions we can draw:

1.  Raw data distribution:

```         
-    The original data (leftmost histogram) appears to be right-skewed, with most values concentrated between 0 and 300.

-    There's a long tail extending to the right, with some values reaching up to 50
```

2.  Min-Max Standardization:

    -   This technique rescales the data to a fixed range, typically \[0, 1\].

    -   The shape of the distribution remains similar to the raw data, preserving the right-skewness.

    -   The x-axis now ranges from 0 to 1, indicating successful normalization of the scale.

3.  Z-score Standardization:

    -   This method transforms the data to have a mean of 0 and a standard deviation of 1.

    -   The distribution shape is maintained, but it's centered around 0.

    -   The x-axis now ranges from about -2 to 2, which is typical for z-scores of normally distributed data.

4.  Comparison of techniques:

    -   Both standardization methods preserve the original shape of the distribution.

    -   Min-Max squeezes the data into a fixed range, while Z-score centers it around zero and expresses values in terms of standard deviations from the mean.

5.  Implications:

    -   The right-skewed nature of the data is consistent across all three histograms, suggesting this is an inherent characteristic of the variable being measured.

    -   Standardization doesn't change the fundamental distribution of the data, but it does make it easier to compare or use in certain statistical analyses.

These standardization techniques are useful for preparing data for machine learning algorithms or for comparing variables that are on different scales. The choice between Min-Max and Z-score standardization would depend on the specific requirements of the subsequent analysis or model.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

### Computing proximty matrix

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
proxmat
```

## Computing hierarachical clustering

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
plot(hclust_ward, cex = 0.6)
```

## Selecting optimal clustering algo

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

With reference to the output above, we can see that Ward's method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward's method will be used.

## Determining optimal clusters

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

```{r}
fviz_gap_stat(gap_stat)
```

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

## Visually driven hierarchical clustering analysis

```{r}
shan_ict_mat <- data.matrix(shan_ict)
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )


```

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
qtm(shan_sf_cluster, "CLUSTER")
```

# Skater approach

```{r}
shan_sp <- as_Spatial(shan_sf)
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
coords <- st_coordinates(
  st_centroid(st_geometry(shan_sf)))
plot(st_geometry(shan_sf), 
     border=grey(.5))
plot(shan.nb,
     coords, 
     col="blue", 
     add=TRUE)
```

## Computing minimum spanning tree

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

```{r}
shan.mst <- mstree(shan.w)
class(shan.mst)
dim(shan.mst)
head(shan.mst)
```

```{r}
plot(st_geometry(shan_sf), 
                 border=gray(.5))
plot.mst(shan.mst, 
         coords, 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

```{r}
str(clust6)
```

```{r}
ccs6 <- clust6$groups
ccs6
table(ccs6)
plot(st_geometry(shan_sf), 
     border=gray(.5))
plot(clust6, 
     coords, 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```

# ClustGeo method for spatially constrained clustering

## Ward like hierarchical clustering: ClustGeo

```{r}
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

## Mapping clusters

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
qtm(shan_sf_ngeo_cluster, "CLUSTER")
```

## Spatially constrained hierarchical clustering

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```

With reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below

```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.3)
```

Next, `cutree()` is used to derive the cluster object.

```{r}
groups <- as.factor(cutree(clustG, k=6))
```

We will then join back the group list with *shan_sf* polygon feature data frame by using the code chunk below.

```{r}
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%   rename(`CLUSTER` = `as.matrix.groups.`)
```

We can now plot the map of the newly delineated spatially constrained clusters.

```{r}
qtm(shan_sf_Gcluster, "CLUSTER")
```

# Visual interpretation of clusters

```{r}
ggplot(data = shan_sf_ngeo_cluster,
       aes(x = CLUSTER, y = RADIO_PR)) +
  geom_boxplot()
```

```{r}
ggparcoord(data = shan_sf_ngeo_cluster, 
           columns = c(17:21), 
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster") +
  facet_grid(~ CLUSTER) + 
  theme(axis.text.x = element_text(angle = 30))
```

The parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.

Note that the `scale` argument of `ggparcoor()` provide several methods to scale the clustering variables. They are:

-   std: univariately, subtract mean and divide by standard deviation.

-   robust: univariately, subtract median and divide by median absolute deviation.

-   uniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.

-   globalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.

-   center: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.

-   centerObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param

There is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.

Last but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.

In the code chunk below, `group_by()` and `summarise()` of dplyr are used to derive mean values of the clustering variables.

```{r}
shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(CLUSTER) %>%
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```
